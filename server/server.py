import os
import threading
import time
import json
import io
from flask import Flask, request, jsonify, render_template_string
import scapy.all as scapy
from kafka import KafkaProducer, KafkaConsumer
import numpy as np
import joblib
from pymongo import MongoClient
from datetime import datetime
import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import pandas as pd
import warnings
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from flask import send_file
from flask_cors import CORS
import base64
import matplotlib.cm as cm
from sklearn.metrics import classification_report
from datetime import datetime, timezone
import firebase_admin
from firebase_admin import credentials, firestore

# Path to your Firebase service account key JSON file
firebase_cred = credentials.Certificate(
    r'E:\MINI_PROJECT2-NIDS\flask-server\firebase\ai-nids.json'
)

# Initialize the app with a service account, granting admin privileges
# Note: No databaseURL needed for Firestore
firebase_admin.initialize_app(firebase_cred)

# Get the Firestore client
firestore_client = firestore.client()

from bson import ObjectId
def convert_objectid_to_str(obj):
    # Recursively convert ObjectId to string
    if isinstance(obj, dict):
        return {k: convert_objectid_to_str(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_objectid_to_str(i) for i in obj]
    elif isinstance(obj, ObjectId):
        return str(obj)
    else:
        return obj
        
def store_in_firestore(data):
    try:
        cleaned_data = convert_objectid_to_str(data)
        firestore_client.collection('attacks').add(cleaned_data)
    except Exception as e:
        print(f"Firestore Error: {e}")



# Suppress the specific warning from scikit-learn
warnings.filterwarnings('ignore', category=UserWarning, message='.*X does not have valid feature names.*')


def generate_pie_chart(data_dict, title):
    import matplotlib.pyplot as plt
    import matplotlib.cm as cm
    import numpy as np
    import io

    labels = []
    sizes = []

    for label, count in data_dict.items():
        if count > 0:
            labels.append(str(label))
            sizes.append(count)

    if not sizes:
        sizes = [1]
        labels = ['No Data']

    total = sum(sizes)

    # Color palette
    colors = cm.tab20(np.linspace(0, 1, len(sizes)))

    fig, ax = plt.subplots(figsize=(8, 6))

    wedges, texts, autotexts = ax.pie(
        sizes,
        autopct=lambda pct: f"{pct:.1f}%" if pct > 0 else "",
        startangle=90,
        colors=colors,
        wedgeprops=dict(width=0.4),  # donut effect
        textprops=dict(color="black", fontsize=9)
    )

    # Annotate each wedge with full details
    for i, (wedge, label, size) in enumerate(zip(wedges, labels, sizes)):
        ang = (wedge.theta2 + wedge.theta1) / 2
        x = np.cos(np.deg2rad(ang))
        y = np.sin(np.deg2rad(ang))
        ax.text(x * 0.7, y * 0.7,
                f"{label}\n{size} ({size/total:.1%})",
                ha='center', va='center', fontsize=8, color='black')

    ax.legend(
        wedges,
        [f"{label}: {count} ({count/total:.1%})" for label, count in zip(labels, sizes)],
        title="Classes",
        loc="center left",
        bbox_to_anchor=(1, 0, 0.5, 1),
        fontsize=9
    )

    ax.axis('equal')  # Equal aspect ratio ensures pie is circular
    plt.title(title, fontsize=14)

    plt.figtext(0.5, 0.01, "Generated by Intrusion Detection System", wrap=True,
                horizontalalignment='center', fontsize=9, color='gray')

    buf = io.BytesIO()
    plt.tight_layout()
    plt.savefig(buf, format='png')
    plt.close(fig)
    buf.seek(0)
    return buf


# ─── Configuration ───────────────────────────────────────────────────────────────
ROOT_DIR = r"E:\MINI_PROJECT2-NIDS\flask-server"

KAFKA_BOOTSTRAP = 'localhost:9092'
KAFKA_INSIDE_DOCKER = 'host.docker.internal:9092'
MONGO_URI = 'mongodb://localhost:27017'

# ─── Flask App ───────────────────────────────────────────────────────────────────
app = Flask(__name__)
CORS(app)

# ─── Shared State ───────────────────────────────────────────────────────────────
components = [
    'packet_producer',
    'processing_consumer',
    'ml_consumer',
    'bert_consumer'
]
logs = {name: [] for name in components}
control_flags = {name: False for name in components}
threads = {}

def append_log(comp, msg):
    ts = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
    entry = f"[{ts}] {msg}"
    logs[comp].append(entry)
    if len(logs[comp]) > 200:
        logs[comp].pop(0)

# ─── MongoDB Setup ──────────────────────────────────────────────────────────────
mongo_client = MongoClient(MONGO_URI)
db = mongo_client.NIDS
raw_packets_coll    = db.raw_packets
flow_features_coll  = db.flow_features
predictions_coll    = db.predictions

# ─── Load ML Models ─────────────────────────────────────────────────────────────
model_paths = {
    'decision_tree':       os.path.join(ROOT_DIR, 'models', 'dt_model.joblib'),
    'logistic_regression': os.path.join(ROOT_DIR, 'models', 'lr_model.joblib'),
    'random_forest':       os.path.join(ROOT_DIR, 'models', 'rf_model.joblib'),
    'xgboost':             os.path.join(ROOT_DIR, 'models', 'xgb_model.joblib')
}
ml_models = {}
for name, path in model_paths.items():
    try:
        ml_models[name] = joblib.load(path)
    except ModuleNotFoundError as e:
        append_log('ml_consumer', f"⚠️ Skipping {name} model: {e}")
        # Optionally delete from feature set so you don't call it later

# ─── Load BERT Model ─────────────────────────────────────────────────────────────
bert_dir      = os.path.join(ROOT_DIR, 'bert', 'distilbert_binary_model', 'content', 'distilbert_binary_model')
bert_tokenizer = DistilBertTokenizer.from_pretrained(bert_dir)
bert_model     = DistilBertForSequenceClassification.from_pretrained(bert_dir)
bert_model.eval()

# ─── Common Feature Order ───────────────────────────────────────────────────────
common_features = [
    "Destination Port", "Flow Duration", "Total Fwd Packets", "Total Length of Fwd Packets",
    "Flow Bytes/s", "Flow Packets/s", "Flow IAT Mean", "Flow IAT Std",
    "Fwd Packet Length Max", "Fwd Packet Length Min", "Fwd Packet Length Mean", "Fwd Packet Length Std",
    "Min Packet Length", "Max Packet Length", "Packet Length Mean", "Packet Length Std",
    "Packet Length Variance", "FIN Flag Count", "SYN Flag Count", "ACK Flag Count"
]

# ─── Packet Producer ─────────────────────────────────────────────────────────────
def packet_producer():
    append_log('packet_producer', "Started")
    producer = KafkaProducer(
        bootstrap_servers=KAFKA_BOOTSTRAP,
        value_serializer=lambda v: json.dumps(v).encode('utf-8')
    )

    def handler(pkt):
        if not control_flags['packet_producer']:
            return
        if pkt.haslayer(scapy.TCP):
            data = {
                "timestamp": time.time(),
                "src": pkt[scapy.IP].src,
                "dst": pkt[scapy.IP].dst,
                "sport": pkt[scapy.TCP].sport,
                "dport": pkt[scapy.TCP].dport,
                "protocol": "TCP",
                "length": len(pkt),
                "flags": pkt[scapy.TCP].sprintf("%TCP.flags%")
            }
            producer.send('packets', data)
            #raw_packets_coll.insert_one(data)
            append_log('packet_producer', f"Stored packet: {data}")

    while control_flags['packet_producer']:
        scapy.sniff(count=1, timeout=1, prn=handler, store=0, filter="tcp")

    producer.close()
    append_log('packet_producer', "Stopped")

# ─── Processing Consumer ─────────────────────────────────────────────────────────
def processing_consumer():
    append_log('processing_consumer', "Started")
    consumer = KafkaConsumer(
        'packets',
        bootstrap_servers=KAFKA_BOOTSTRAP,
        value_deserializer=lambda m: json.loads(m.decode('utf-8')),
        auto_offset_reset='earliest',
        enable_auto_commit=True
    )
    producer = KafkaProducer(
        bootstrap_servers=KAFKA_BOOTSTRAP,
        value_serializer=lambda v: json.dumps(v,  default=str).encode('utf-8')
    )
    buffer = {}

    def flow_key(p): 
        return f"{p['src']}_{p['dst']}_{p['sport']}_{p['dport']}_{p['protocol']}"

    def process_flow(flow):
        ts = sorted(p["timestamp"] for p in flow)
        iats = np.diff(ts) if len(ts)>1 else [0]
        duration = max(ts)-min(ts)
        lengths = [p["length"] for p in flow]
        stats = {
                "src": flow[0]["src"],
                "Destination Port": flow[0]["dport"],
                "Flow Duration": duration,
                "Total Fwd Packets": len(flow),
                "Total Length of Fwd Packets": sum(lengths),
                "Flow Bytes/s": sum(lengths)/(duration+1e-6),
                "Flow Packets/s": len(flow)/(duration+1e-6),
                "Flow IAT Mean": float(np.mean(iats)),
                "Flow IAT Std": float(np.std(iats)),
                "Fwd Packet Length Max": int(np.max(lengths)),
                "Fwd Packet Length Min": int(np.min(lengths)),
                "Fwd Packet Length Mean": float(np.mean(lengths)),
                "Fwd Packet Length Std": float(np.std(lengths)),
                "Min Packet Length": int(np.min(lengths)),
                "Max Packet Length": int(np.max(lengths)),
                "Packet Length Mean": float(np.mean(lengths)),
                "Packet Length Std": float(np.std(lengths)),
                "Packet Length Variance": float(np.var(lengths)),
                "FIN Flag Count": sum('F' in p["flags"] for p in flow),
                "SYN Flag Count": sum('S' in p["flags"] for p in flow),
                "ACK Flag Count": sum('A' in p["flags"] for p in flow)
            }

        #flow_features_coll.insert_one(stats)
        append_log('processing_consumer', f"Stored flow: {stats}")
        producer.send("flow_features", stats)

    while control_flags['processing_consumer']:
        recs = consumer.poll(timeout_ms=1000)
        for msgs in recs.values():
            for msg in msgs:
                pkt = msg.value
                key = flow_key(pkt)
                buffer.setdefault(key, []).append(pkt)
                if len(buffer[key]) >= 5:
                    flow = buffer.pop(key)
                    process_flow(flow)

    consumer.close()
    producer.close()
    append_log('processing_consumer', "Stopped")

# ─── ML Consumer ─────────────────────────────────────────────────────────────────
model_class_counts = {
    name: {k: 0 for k in range(15)} for name in ml_models.keys()
}
model_class_counts["bert"] = {k: 0 for k in range(2)}

def ml_consumer():
    append_log('ml_consumer', "Started")
    consumer = KafkaConsumer(
        'flow_features',
        bootstrap_servers=KAFKA_BOOTSTRAP,
        value_deserializer=lambda m: json.loads(m.decode('utf-8')),
        auto_offset_reset='earliest',
        enable_auto_commit=True
    )

    while control_flags['ml_consumer']:
        recs = consumer.poll(timeout_ms=1000)
        for msgs in recs.values():
            for msg in msgs:
                feats = msg.value
                vec = [feats.get(f, 0) for f in common_features]

                preds = {}
                is_attack = False
                for model_name, model in ml_models.items():
                    pred = int(model.predict([vec])[0])
                    preds[model_name] = pred
                    if pred != 0:  # Only class 0 is BENIGN
                        is_attack = True
                    if pred in model_class_counts[model_name]:
                        model_class_counts[model_name][pred] += 1
                    else:
                        model_class_counts[model_name][pred] = 1

                if is_attack:
                    document = {
                        'timestamp': datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S %Z'),
                        'source_ip': feats.get('src'),  # requires adding src to flow feats
                        'features': feats,
                        'predictions': preds
                    }
                    predictions_coll.insert_one(document)
                    store_in_firestore(document)
                    append_log('ml_consumer', f"[ATTACK] Stored prediction: {preds}")
                else:
                    append_log('ml_consumer', f"[BENIGN] Skipped storing benign flow")

    consumer.close()
    append_log('ml_consumer', "Stopped")


# ─── BERT Consumer ───────────────────────────────────────────────────────────────
bert_class_counts = {0: 0, 1: 0}

def bert_consumer():
    append_log('bert_consumer', "Started")
    consumer = KafkaConsumer(
        'flow_features',
        bootstrap_servers=KAFKA_BOOTSTRAP,
        value_deserializer=lambda m: json.loads(m.decode('utf-8')),
        auto_offset_reset='earliest',
        enable_auto_commit=True
    )

    while control_flags['bert_consumer']:
        recs = consumer.poll(timeout_ms=1000)
        for msgs in recs.values():
            for msg in msgs:
                feats = msg.value
                vec = [feats.get(f, 0) for f in common_features]
                text_input = " ".join(str(x) for x in vec)

                tokens = bert_tokenizer(text_input, return_tensors="pt", truncation=True, padding=True)
                with torch.no_grad():
                    output = bert_model(**tokens)
                    pred = int(torch.argmax(output.logits, dim=1).item())
                    bert_class_counts[pred] += 1
                    model_class_counts["bert"][pred] += 1

                if pred == 1:
                    doc = {
                        'timestamp': datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S %Z'),
                        'source_ip': feats.get('src'),
                        'features': feats,
                        'bert_prediction': pred
                    }
                    predictions_coll.insert_one(doc)
                    store_in_firestore(doc)
                    append_log('bert_consumer', f"[ATTACK] BERT prediction = {pred}")
                else:
                    append_log('bert_consumer', f"[BENIGN] Skipped storing benign flow")

    consumer.close()
    append_log('bert_consumer', "Stopped")

# ─── Controller Helpers ─────────────────────────────────────────────────────────
def start_component(name, fn):
    if control_flags[name]:
        return
    control_flags[name] = True
    th = threading.Thread(target=fn, daemon=True)
    threads[name] = th
    th.start()

def stop_component(name):
    control_flags[name] = False

# ─── Flask Routes ───────────────────────────────────────────────────────────────
@app.route('/')
def home():
    return render_template_string("""
      <h1>NIDS Control API</h1>
      <ul>
        <li><a href="/test">test</a></li>
        <li><a href="/logs/packet_producer">Logs - Packet Producer</a></li>
        <li><a href="/logs/processing_consumer">Logs - Processing Consumer</a></li>
        <li><a href="/logs/ml_consumer">Logs - ML Consumer</a></li>
        <li><a href="/logs/bert_consumer">Logs - BERT Consumer</a></li>
        <li><a href="/test-insert">insert</a></li>
        <li><a href="/api/stats">API Stats</a></li>
        <li><a href="/api/upload">Upload CSV</a></li>
      </ul>
    """.strip())

@app.route('/start_all', methods=['POST'])
def start_all():
    for name, fn in [
        ('packet_producer', packet_producer),
        ('processing_consumer', processing_consumer),
        ('ml_consumer', ml_consumer),
        ('bert_consumer', bert_consumer)
    ]:
        start_component(name, fn)
    return jsonify(status="started_all")

@app.route('/stop_all', methods=['POST'])
def stop_all():
    for name in components:
        stop_component(name)
    return jsonify(status="stopped_all")

@app.route('/start/<name>', methods=['POST'])
def start_one(name):
    mapping = {
        'packet_producer': packet_producer,
        'processing_consumer': processing_consumer,
        'ml_consumer': ml_consumer,
        'bert_consumer': bert_consumer
    }
    if name in mapping:
        start_component(name, mapping[name])
        return jsonify(status=f"started_{name}")
    return jsonify(error="unknown_component"), 404

@app.route('/stop/<name>', methods=['POST'])
def stop_one(name):
    if name in components:
        stop_component(name)
        return jsonify(status=f"stopped_{name}")
    return jsonify(error="unknown_component"), 404

@app.route('/logs/<name>')
def get_logs(name):
    if name in logs:
        return jsonify(logs=logs[name])
    return jsonify(error="unknown_component"), 404

# ─── Stats for Charts ───────────────────────────────────────────────────────────
@app.route('/api/stats/classwise')
def api_classwise_stats():
    # Prepare a safe-to-serialize copy (keys as strings)
    serialized_counts = {
        model: {str(cls): count for cls, count in classes.items()}
        for model, classes in model_class_counts.items()
    }
    return jsonify(serialized_counts)

@app.route('/api/stats')
def api_stats():
    total_pkts  = raw_packets_coll.count_documents({})
    attack_pkts = predictions_coll.count_documents({'is_attack': True})
    return jsonify(total_packets=total_pkts, attack_packets=attack_pkts)


@app.route("/chart/<model_name>")
def get_chart(model_name):
    if model_name == "bert":
        buf = generate_pie_chart(bert_class_counts, "BERT Predictions")
    elif model_name in model_class_counts:
        buf = generate_pie_chart(model_class_counts[model_name], f"{model_name.upper()} Predictions")
    else:
        return jsonify({"error": "Model not found"}), 404

    return send_file(buf, mimetype='image/png')

# ───CSV Upload/test endpoin t───────────────────────────────────────────────────────────
import numpy as np

@app.route('/api/upload', methods=['POST'])
def api_upload():
    if 'file' not in request.files:
        return jsonify(error="no_file"), 400

    f = request.files['file']
    try:
        df = pd.read_csv(io.StringIO(f.read().decode('utf-8')))
    except Exception as e:
        return jsonify(error="invalid_csv", detail=str(e)), 400

    # Check for label column
    if 'label' not in df.columns:
        return jsonify(error="missing_label_column"), 400

    labels = df['label'].astype(int).tolist()
    df = df.drop(columns=['label'])

    results = []
    predictions_summary = {name: [] for name in ml_models}
    predictions_summary['bert'] = []

    # Convert feature columns to numeric, replace invalid parsing with 0
    df[common_features] = df[common_features].apply(pd.to_numeric, errors='coerce').fillna(0)

    for _, row in df.iterrows():
        feats = {col: row.get(col, 0) for col in common_features}
        vec = [feats[f] for f in common_features]

        # Convert vec to numpy array and clean infinite/NaN values
        vec_array = np.array(vec, dtype=np.float32)
        if not np.isfinite(vec_array).all():
            vec_array = np.nan_to_num(vec_array, nan=0.0, posinf=0.0, neginf=0.0)

        # ML model predictions
        ml_preds = {}
        for name, model in ml_models.items():
            pred = int(model.predict([vec_array])[0])
            ml_preds[name] = pred
            predictions_summary[name].append(pred)

        # BERT prediction
        txt = " ".join(f"{k}:{feats[k]}" for k in common_features)
        inputs = bert_tokenizer(txt, return_tensors="pt", truncation=True, padding=True)
        with torch.no_grad():
            logits = bert_model(**inputs).logits
        bert_pred = int(torch.argmax(torch.softmax(logits, dim=-1), dim=-1))
        predictions_summary['bert'].append(bert_pred)

        results.append({
            'features': feats,
            'ml_predictions': ml_preds,
            'bert_prediction': bert_pred
        })

    # Calculate metrics
    metrics = {}
    for model_name, preds in predictions_summary.items():
        report = classification_report(labels, preds, output_dict=True, zero_division=0)
        metrics[model_name] = report

    # Generate pie charts
    charts = {}
    for model_name, preds in predictions_summary.items():
        counts = pd.Series(preds).value_counts().to_dict()
        chart_buf = generate_pie_chart(counts, title=f"{model_name} Predictions")
        encoded = base64.b64encode(chart_buf.read()).decode('utf-8')
        charts[model_name] = f"data:image/png;base64,{encoded}"

    return jsonify(
        metrics=metrics,
        charts=charts
    )
# ─── Testing UI Endpoint ───────────────────────────────────────────────────────
@app.route('/test')
def test_ui():
    return """
<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>NIDS Pipeline Test</title>
</head>
<body style="font-family: sans-serif; padding: 2rem;">
  <h1>Pipeline Control (Test Page)</h1>
  <button id="start-btn" style="padding:1rem; font-size:1rem;">🚀 Start Pipeline</button>
  <button id="stop-btn"  style="padding:1rem; font-size:1rem; margin-left:1rem;">🛑 Stop Pipeline</button>
  <p id="status" style="margin-top:1rem; color: green;"></p>
  <script>
    async function callApi(path) {
      document.getElementById('status').textContent = 'Waiting for response...';
      const res = await fetch(path, { method: 'POST' });
      const json = await res.json();
      document.getElementById('status').textContent = JSON.stringify(json);
    }
    document.getElementById('start-btn').onclick = () => callApi('/start_all');
    document.getElementById('stop-btn').onclick  = () => callApi('/stop_all');
  </script>
</body>
</html>
    """

@app.route('/test-insert', methods=['GET'])
def test_insert():
    test_doc = {
        "timestamp": datetime.utcnow(),
        "message": "This is a test insert"
    }
    try:
        result = predictions_coll.insert_one(test_doc)
        return jsonify({"status": "success", "inserted_id": str(result.inserted_id)})
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)})



if __name__ == '__main__':
    # Run on all interfaces, admin mode required for scapy sniffing
    app.run(debug=True, host='0.0.0.0')
